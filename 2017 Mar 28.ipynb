{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### MA755 Machine Learning - Ensemble Learning - 28 Mar 2017 \n",
    "\n",
    "These notes are based on, and include images from, [_Hands-On Machine Learning with Scikit-Learn and TensorFlow_](http://shop.oreilly.com/product/0636920052289.do)\n",
    "- by Aurélien Géron\n",
    "- Published by O'Reilly Media, Inc., 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics         as sk_me\n",
    "import sklearn.model_selection as sk_ms\n",
    "import sklearn.datasets        as sk_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sk_ds.load_iris()\n",
    "(iris.data.shape, \n",
    " iris.target.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Store the feature/variable names for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iris_feature_names = iris[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 4), (75,), (75, 4), (75,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data,   test_data,\n",
    " train_target, test_target\n",
    " ) = sk_ms.train_test_split(iris.data, \n",
    "                            iris.target, \n",
    "                            test_size=0.5, \n",
    "                            random_state=42)\n",
    "(train_data.shape, train_target.shape, \n",
    " test_data.shape,  test_target.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### Chapter 7. Ensemble Learning and Random Forests\n",
    "\n",
    "Below we describe several types of ensemble classifiers. \n",
    "\n",
    "__Hard voting__. Train multiple classifiers on the entire training dataset. For each instance choose the most common class prediction returned by the multiple classifiers. \n",
    "\n",
    "__Soft voting__. Train multiple classifiers on the entire training dataset. For each instance choose the class prediction of the most confident (highest probability) classifier, for that instance.\n",
    "\n",
    "__Bagging__. Train a single classifier on multiple training subsets, created by sampling instances __with__ replacement. Choose the most common class prediction.\n",
    "\n",
    "__Pasting__. Train a single classifier on multiple training subsets, created by sampling instances __without__ replacement. Choose the most common class prediction. \n",
    "\n",
    "__Random Subspaces__. Train a single classifier on multiple subsets, created by sampling the features used, but including all instances. Choose the most common class prediction.\n",
    "\n",
    "__Random Patches__. Train a single classifier on multiple subsets, created by sampling features and sampling instances. Choose the most common class prediction.\n",
    "\n",
    "__AdaBoost__. Train a single classifier (i.e. decision tree) iteratively on subsets that are (by design) likely to contain instances that were previously incorrectly classified.\n",
    "\n",
    "__Gradient Boosting__. Seems to be regression only. \n",
    "\n",
    "__Stacking__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hard voting classifier \n",
    "\n",
    "Train multiple classifiers on the entire training dataset. \n",
    "\n",
    "For each instance choose the most common class prediction returned by the multiple classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hard voting - example\n",
    "\n",
    "Create four base classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree         import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm          import SVC, LinearSVC\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "sv_clf = SVC(probability=True)\n",
    "ls_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These classifiers are used at several points below as parts of the ensemble classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create an ensemble, __hard voting__, classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('dt', Decisio...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))],\n",
       "         n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble     import VotingClassifier\n",
    "\n",
    "vo_clf = VotingClassifier(\n",
    "        estimators=[('lr', lr_clf), \n",
    "                    ('dt', dt_clf), \n",
    "                    ('ls', ls_clf)],\n",
    "        voting='hard'\n",
    "    )\n",
    "vo_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare the prediction accuracy of this ensemble classifier and of its base classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973 LogisticRegression\n",
      "0.933 DecisionTreeClassifier\n",
      "0.987 SVC\n",
      "0.987 VotingClassifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (lr_clf, dt_clf, sv_clf, vo_clf):\n",
    "    clf.fit(train_data, train_target)\n",
    "    test_predict = clf.predict(test_data)\n",
    "    print(round(accuracy_score(test_target, \n",
    "                               test_predict),\n",
    "                3),\n",
    "         clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Soft voting classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create an ensemble, __soft voting__, classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('dt', Decisio...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vo_clf = VotingClassifier(\n",
    "        estimators=[('lr', lr_clf), \n",
    "                    ('dt', dt_clf), \n",
    "                    ('sv', sv_clf)],\n",
    "        voting='soft'\n",
    "    )\n",
    "vo_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare the prediction accuracy of this ensemble classifier and of its base classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973 LogisticRegression\n",
      "0.947 DecisionTreeClassifier\n",
      "0.987 SVC\n",
      "0.96 VotingClassifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (lr_clf, dt_clf, sv_clf, vo_clf):\n",
    "    clf.fit(train_data, \n",
    "            train_target)\n",
    "    test_predict = clf.predict(test_data)\n",
    "    print(round(accuracy_score(test_target, \n",
    "                               test_predict),\n",
    "                3),\n",
    "          clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bagging and pasting - `BaggingClassifier`\n",
    "\n",
    "The `BaggingClassifier` implements both the bagging and pasting techniques. \n",
    "\n",
    "It automatically performs __soft voting__ if the base classifiers can estimate class probabilities. \n",
    "\n",
    "Otherwise it performs __hard voting__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bagging\n",
    "\n",
    "- Trains a single classifier on multiple training subsets\n",
    "- Creates these subsets by sampling instances __with__ replacement\n",
    "- Chooses the most common class prediction\n",
    "\n",
    "See http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Parameters:\n",
    "\n",
    "- `bootstrap=True` indicates that __bagging__ is to be performed\n",
    "- `n_estimators=5` indicates that 5 decision trees (base classifiers) are created\n",
    "- `max_samples=10` indicates that the size of the subset to create (and use for training the base classifiers)\n",
    "- `n_jobs=-1` indicates that all available cpus are used to train the base classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the classifier and train it on the training datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=10, n_estimators=5, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), bootstrap=True, \n",
    "                            n_estimators=5, max_samples=10, n_jobs=-1)\n",
    "bag_clf.fit(train_data, \n",
    "            train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create predictions from `test_data` and check their accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging (with replacement sampling)\n",
      "Accuracy score:  0.96\n"
     ]
    }
   ],
   "source": [
    "test_predict = bag_clf.predict(test_data)\n",
    "print(\"Bagging (with replacement sampling)\")\n",
    "print(\"Accuracy score: \", accuracy_score(test_target, \n",
    "                                         test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pasting\n",
    "\n",
    "- Train a single classifier on multiple training subsets\n",
    "- Create these subsets by sampling instances __without__ replacement\n",
    "- Choose the most common class prediction\n",
    "\n",
    "See http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Parameters:\n",
    "\n",
    "- `bootstrap=False` indicates that __pasting__ is to be performed\n",
    "- `n_estimators=5` indicates that 5 decision trees (base classifiers) are created\n",
    "- `max_samples=10` indicates that the size of the subset to create (and use for training the base classifiers)\n",
    "- `n_jobs=-1` indicates that all available cpus are used to train the base classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the classifier and train it on the training datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "         bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=10, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), bootstrap=False, \n",
    "                            n_estimators=500, max_samples=10, n_jobs=-1\n",
    "    )\n",
    "bag_clf.fit(train_data, \n",
    "            train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create predictions from `test_data` and check their accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasting (without replacement)\n",
      "Accuracy score:  0.986666666667\n"
     ]
    }
   ],
   "source": [
    "test_predict = bag_clf.predict(test_data)\n",
    "print(\"Pasting (without replacement)\")\n",
    "print(\"Accuracy score: \", accuracy_score(test_target, \n",
    "                                         test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Out-of-bag error\n",
    "\n",
    "When using bagging, not all instances are used to train the data because sampling is performed __with replacement__. These unused instances can be used to evaluate the classifier since they are not part of the training dataset. \n",
    "\n",
    "__Out-of-bag error__ is the error rate on these unused instances. \n",
    "\n",
    "See https://en.wikipedia.org/wiki/Out-of-bag_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Call `BaggingClassifier` with parameter `oob_score=True` and then fit to the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=10, n_estimators=500, n_jobs=-1, oob_score=True,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), bootstrap=True, \n",
    "                            n_estimators=500, max_samples=10, n_jobs=-1, \n",
    "                            oob_score=True\n",
    ")\n",
    "bag_clf.fit(train_data, \n",
    "            train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The out-of-bag score is available in `oob_score_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-bag score:  0.906666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Out-of-bag score: \", bag_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compare this to the model accuracy on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.986666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_predict = bag_clf.predict(test_data)\n",
    "\n",
    "print(\"Accuracy score: \", \n",
    "      accuracy_score(test_target, \n",
    "                     test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### `RandomForestClassifier`\n",
    "\n",
    "The Random Forest model is a bagging classifier that uses decision trees. \n",
    "\n",
    "Below two classifiers are created and evaluated. \n",
    "\n",
    "The first is `RandomClassifier` and the second uses `BaggingClassifier` and `DecisionTreeClassifier` to implement the random forest model.\n",
    "\n",
    "See http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Parameters:\n",
    "\n",
    "- `bootstrap=False` indicates that __pasting__ is to be performed\n",
    "- `n_estimators=5` indicates that 5 decision trees (base classifiers) are created\n",
    "- `max_samples=10` indicates that the size of the subset to create (and use for training the base classifiers)\n",
    "- `n_jobs=-1` indicates that all available cpus are used to train the base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for RandomForestClassifier:  0.973333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_features='auto', n_jobs=-1\n",
    "                                )\n",
    "rnd_clf.fit(train_data, \n",
    "            train_target)\n",
    "print(\"Accuracy score for RandomForestClassifier: \", \n",
    "      accuracy_score(test_target,\n",
    "                     rnd_clf.predict(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for BaggingClassifier:  0.986666666667\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(splitter=\"random\", max_features='auto'),\n",
    "        n_estimators=500, bootstrap=True, n_jobs=-1\n",
    "    )\n",
    "bag_clf.fit(train_data, train_target)\n",
    "print(\"Accuracy score for BaggingClassifier: \", \n",
    "      accuracy_score(test_target,\n",
    "                     bag_clf.predict(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The two sets of predictions are nearly identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29,  0,  0],\n",
       "       [ 0, 24,  1],\n",
       "       [ 0,  0, 21]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_me.confusion_matrix(rnd_clf.predict(test_data),\n",
    "                       bag_clf.predict(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Decision trees place the most effective variable (at predicting the target variable) at the root of the tree. \n",
    "\n",
    "As random forests create many decision trees, we can measure the importance of a variable by the number of times it is the split variable at the root of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0948241 ,  0.04750261,  0.44927531,  0.40839798])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.0948241032171\n",
      "sepal width (cm) 0.0475026060019\n",
      "petal length (cm) 0.449275310455\n",
      "petal width (cm) 0.408397980326\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], \n",
    "                       rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "1. Set the row weights identically to $1/m$, where $m$ is the number of rows.\n",
    "1. Create a subset of the rows using the row weights as probabilities.\n",
    "1. Train the classifier on that subset.\n",
    "1. Make predictions with that classifier.\n",
    "1. Weight the classifier; higher weights correspond to less error\n",
    "1. Add this weighted classifier to the overall prediction function\n",
    "1. Updated row weights: incorrectly predicted rows are weighted higher\n",
    "1. Go to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for AdaBoostClassifier:  0.946666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), \n",
    "                             n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5\n",
    "    )\n",
    "ada_clf.fit(train_data, \n",
    "            train_target)\n",
    "print(\"Accuracy score for AdaBoostClassifier: \", \n",
    "      accuracy_score(test_target,\n",
    "                     ada_clf.predict(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
